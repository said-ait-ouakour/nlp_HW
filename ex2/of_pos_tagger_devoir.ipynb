{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **Creation of Simple Pos Tagging** "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xyq7e85wv1TI"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import conllu\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C4QHn4Tiv1TK"
      },
      "outputs": [],
      "source": [
        "#loading the training corpus\n",
        "\n",
        "# Open the .conllu file\n",
        "with open(\"train.conllu\", \"r\", encoding=\"utf-8\") as file:\n",
        "    # Read the contents of the file\n",
        "    data = file.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GAcE9mIVv1TK"
      },
      "outputs": [],
      "source": [
        "#loading the test corpus\n",
        "#open the .conllu file\n",
        "\n",
        "with open(\"test.conllu\",\"r\",encoding=\"utf-8\") as f:\n",
        "    #read the content\n",
        "    content = f.read()\n",
        "with open(\"test.pos\",\"w\",encoding=\"utf-8\") as file:\n",
        "    sen = conllu.parse(content)\n",
        "    for s in sen:\n",
        "        for t in s:\n",
        "            word=t[\"form\"]\n",
        "            ps = t[\"upostag\"]\n",
        "            file.write(word + '\\t' + ps +'\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FCyoggSev1TK"
      },
      "outputs": [],
      "source": [
        "# Parse the .conllu data\n",
        "import conllu\n",
        "sentences = conllu.parse(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svoaxfY8v1TL"
      },
      "outputs": [],
      "source": [
        "with open(\"train.pos\", 'w', encoding='utf-8') as file:\n",
        "    for sentence in sentences:\n",
        "    # Iterate over each token in the sentence\n",
        "        for token in sentence:\n",
        "        # Access token properties such as word form, lemma, part of speech, etc.\n",
        "            word_form = token[\"form\"]\n",
        "            pos = token[\"upostag\"]\n",
        "            file.write(word_form + '\\t' + pos +'\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "alilzCekv1TL"
      },
      "outputs": [],
      "source": [
        "with open(r\".\\train.conllu\", \"r\", encoding=\"utf-8\") as f:\n",
        "    train_data = f.read()\n",
        "with open(r\".\\dev.conllu\", \"r\", encoding=\"utf-8\") as f:\n",
        "    test_data = f.read()\n",
        "train = {}\n",
        "sentences = conllu.parse(train_data)\n",
        "\n",
        "for sentence in sentences:\n",
        "    for token in sentence:\n",
        "        train[token['form']] = token['upos']\n",
        "test = {}\n",
        "sentences = conllu.parse(test_data)\n",
        "\n",
        "for sentence in sentences:\n",
        "    for token in sentence:\n",
        "        test[token['form']] = token['upos']\n",
        "vocab_l=list()\n",
        "sentences = conllu.parse(train_data)\n",
        "\n",
        "for sentence in sentences:\n",
        "    for token in sentence:\n",
        "        vocab_l.append(token['form'])\n",
        "vocab_l=vocab_l+list(test.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zoJ2oI1Kv1TM",
        "outputId": "6bd09c0d-25b9-48ec-e34d-746b8a59fca4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "373634"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(vocab_l)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "QHxX2jJ2v1TN"
      },
      "source": [
        "START"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7IN90Er0v1TN",
        "outputId": "df34c22a-1401-4cbb-b672-33aa9a3bde27"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A few items of the training corpus list\n",
            "['Les\\tDET\\n', 'commotions\\tNFP\\n', 'cérébrales\\tADJFP\\n', 'sont\\tAUX\\n', 'devenu\\tVPPMS\\n']\n"
          ]
        }
      ],
      "source": [
        "#loding train data\n",
        "with open(\"train.pos\", 'r',encoding='utf-8') as f:\n",
        "    training_corpus = f.readlines()\n",
        "\n",
        "print(f\"A few items of the training corpus list\")\n",
        "print(training_corpus[0:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GBYPefslv1TO",
        "outputId": "8097fee2-722d-4210-f026-3a506f4cde1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary dictionary, key is the word, value is a unique integer\n",
            "!:0\n",
            "\":1\n",
            "#:2\n",
            "$:3\n",
            "%:4\n",
            "&:5\n",
            "':6\n",
            "'':7\n",
            "'06:8\n",
            "'07:9\n",
            "'900:10\n",
            "'upa'upa:11\n",
            "(:12\n",
            "):13\n",
            "*:14\n",
            "+:15\n",
            "+0,5:16\n",
            "+0,6:17\n",
            "+1:18\n",
            "+1,8:19\n",
            "+14:20\n"
          ]
        }
      ],
      "source": [
        "# vocab: dictionary that has the index of the corresponding words\n",
        "vocab = {} \n",
        "\n",
        "# Get the index of the corresponding words. \n",
        "for i, word in enumerate(sorted(set(vocab_l))): \n",
        "    vocab[word] = i       \n",
        "    \n",
        "print(\"Vocabulary dictionary, key is the word, value is a unique integer\")\n",
        "cnt = 0\n",
        "for k,v in vocab.items():\n",
        "    print(f\"{k}:{v}\")\n",
        "    cnt += 1\n",
        "    if cnt > 20:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l8nr5do3v1TO",
        "outputId": "556afd8a-c19f-4ab7-8af4-30f8f208ade4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "44929"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nQh-lBSNv1TO",
        "outputId": "c1b69430-f5a4-4b5a-fb3e-4273923308a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A sample of the test corpus\n",
            "['Je\\tPPER1S\\n', 'sens\\tVERB\\n', \"qu'\\tCOSUB\\n\", 'entre\\tPREP\\n', 'Ã§a\\tPDEMMS\\n', 'et\\tCOCO\\n', 'les\\tDET\\n', 'films\\tNMP\\n', 'de\\tPREP\\n', 'mÃ©decins\\tNMP\\n']\n"
          ]
        }
      ],
      "source": [
        "# load in the test corpus\n",
        "with open(\"test.pos\", 'r') as f:\n",
        "    y = f.readlines()\n",
        "\n",
        "print(\"A sample of the test corpus\")\n",
        "print(y[0:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZzXFeumav1TO"
      },
      "outputs": [],
      "source": [
        "# testing words\n",
        "with open(\"test.pos\",\"r\") as f:\n",
        "    lines=f.readlines()\n",
        "\n",
        "    first_column_words = []\n",
        "\n",
        "# Parcourir chaque ligne et extraire le mot de la première colonne\n",
        "for line in lines:\n",
        "    # Diviser la ligne en colonnes en utilisant '\\t' comme séparateur\n",
        "    columns = line.split('\\t')\n",
        "\n",
        "    # Vérifier si le nombre de colonnes est suffisant\n",
        "    if len(columns) >= 1:\n",
        "        # Ajouter le mot de la première colonne à la liste\n",
        "        first_column_words.append(columns[0])\n",
        "\n",
        "# Ouvrir le fichier de sortie en écriture\n",
        "with open('testing.words', 'w') as output_file:\n",
        "    # Écrire les mots de la première colonne dans le fichier de sortie\n",
        "    for w in first_column_words:\n",
        "        output_file.write(w + '\\n')\n",
        "        \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OG6VFSzfv1TP"
      },
      "outputs": [],
      "source": [
        "\n",
        "def preprocess(vocab, data_fp):\n",
        "    \"\"\"\n",
        "    Preprocess data\n",
        "    \"\"\"\n",
        "    orig = []\n",
        "    prep = []\n",
        "\n",
        "    # Read data\n",
        "    with open(data_fp, \"r\") as data_file:\n",
        "\n",
        "        for cnt, word in enumerate(data_file):\n",
        "\n",
        "            # End of sentence\n",
        "            if not word.split():\n",
        "                orig.append(word.strip())\n",
        "                word = \"--n--\"\n",
        "                prep.append(word)\n",
        "                continue\n",
        "\n",
        "            # Handle unknown words\n",
        "            elif word.strip() not in vocab:\n",
        "                orig.append(word.strip())\n",
        "                word = assign_unk(word)\n",
        "                prep.append(word)\n",
        "                continue\n",
        "\n",
        "            else:\n",
        "                orig.append(word.strip())\n",
        "                prep.append(word.strip())\n",
        "\n",
        "    assert(len(orig) == len(open(data_fp, \"r\").readlines()))\n",
        "    assert(len(prep) == len(open(data_fp, \"r\").readlines()))\n",
        "\n",
        "    return orig, prep\n",
        "def assign_unk(word):\n",
        "    return \"<UNK>\"\n",
        "\n",
        "def get_word_tag(line, vocabulary):\n",
        "    if not line.strip():\n",
        "        word = \"--n--\"\n",
        "        tag = \"--s--\"\n",
        "        return word, tag\n",
        "    else:\n",
        "        parts = line.split(\"\\t\", 1)\n",
        "        if len(parts) != 2:\n",
        "            return None, None\n",
        "        word, tag = parts\n",
        "        if word not in vocabulary: \n",
        "            word = assign_unk(word)\n",
        "        return word, tag.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_QtEA2qFv1TP",
        "outputId": "00f8d250-c457-44df-8a7f-8b69b719658f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The length of the preprocessed test corpus:  10298\n",
            "This is a sample of the test_corpus: \n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "10298"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#corpus without tags, preprocessed\n",
        "_, prep = preprocess(vocab, \"testing.words\")     \n",
        "\n",
        "print('The length of the preprocessed test corpus: ', len(prep))\n",
        "print('This is a sample of the test_corpus: ')\n",
        "len(prep)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "4MZigtwGv1TP"
      },
      "source": [
        "**TRAINING**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lsgB0Q6Fv1TP"
      },
      "outputs": [],
      "source": [
        "def create_dictionaries(training_corpus, vocab):\n",
        "    \"\"\"\n",
        "    Input: \n",
        "        training_corpus: a corpus where each line has a word followed by its tag.\n",
        "        vocab: a dictionary where keys are words in vocabulary and value is an index\n",
        "    Output: \n",
        "        emission_counts: a dictionary where the keys are (tag, word) and the values are the counts\n",
        "        transition_counts: a dictionary where the keys are (prev_tag, tag) and the values are the counts\n",
        "        tag_counts: a dictionary where the keys are the tags and the values are the counts\n",
        "    \"\"\"\n",
        "\n",
        "    from collections import defaultdict\n",
        "    # initialize the dictionaries using defaultdict\n",
        "    emission_counts = defaultdict(int)\n",
        "    transition_counts = defaultdict(int)\n",
        "    tag_counts = defaultdict(int)\n",
        "    \n",
        "    # Initialize \"prev_tag\" (previous tag) with the start state, denoted by '--s--'\n",
        "    prev_tag = '--s--' \n",
        "    \n",
        "    # use 'i' to track the line number in the corpus\n",
        "    i = 0 \n",
        "    \n",
        "    # Each item in the training corpus contains a word and its POS tag\n",
        "    # Go through each word and its tag in the training corpus\n",
        "    for word_tag in training_corpus: \n",
        "        \n",
        "        # Increment the word_tag count\n",
        "        i += 1\n",
        "        \n",
        "        # Every 50,000 words, print the word count\n",
        "        if i % 50000 == 0:\n",
        "            print(f\"word count = {i}\")\n",
        "            \n",
        "        ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
        "        # get the word and tag using the get_word_tag helper function (imported from utils_pos.py)\n",
        "        word, tag = get_word_tag(word_tag,vocab) \n",
        "        \n",
        "        # Increment the transition count for the previous word and tag\n",
        "        transition_counts[(prev_tag, tag)] += 1\n",
        "        \n",
        "        # Increment the emission count for the tag and word\n",
        "        emission_counts[(tag, word)] += 1\n",
        "\n",
        "        # Increment the tag count\n",
        "        tag_counts[tag] += 1\n",
        "\n",
        "        # Set the previous tag to this tag (for the next iteration of the loop)\n",
        "        prev_tag = tag\n",
        "        \n",
        "        ### END CODE HERE ###\n",
        "        \n",
        "    return emission_counts, transition_counts, tag_counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v35G7z1Yv1TQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5GJ0ys_vv1TQ",
        "outputId": "379c7f39-d20d-4634-e2df-1e1c90981300"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "word count = 50000\n",
            "word count = 100000\n",
            "word count = 150000\n",
            "word count = 200000\n",
            "word count = 250000\n",
            "word count = 300000\n",
            "word count = 350000\n"
          ]
        }
      ],
      "source": [
        "emission_counts, transition_counts, tag_counts = create_dictionaries(training_corpus, vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "X0oBUPw5v1TQ",
        "outputId": "30b7c572-7699-48cf-9326-10b17689dc27"
      },
      "outputs": [],
      "source": [
        "# emission_counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "wmMla1Eqv1TR",
        "outputId": "98a5b677-73ef-4a49-b21b-1937cabac655"
      },
      "outputs": [],
      "source": [
        "# tag_counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "7ynUB-ZPv1TR",
        "outputId": "ec5dc841-28e1-46ee-e724-4348c15902de"
      },
      "outputs": [],
      "source": [
        "# transition_counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AEKp3X9ev1TR",
        "outputId": "b8d067ca-17a8-4f03-ab03-44cd6162a9bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of POS tags (number of 'states'): 66\n",
            "View these POS tags (states)\n",
            "['ADJ', 'ADJFP', 'ADJFS', 'ADJMP', 'ADJMS', 'ADV', 'AUX', 'CHIF', 'COCO', 'COSUB', 'DET', 'DETFS', 'DETMS', 'DINTFS', 'DINTMS', 'INTJ', 'MOTINC', 'NFP', 'NFS', 'NMP', 'NMS', 'NOUN', 'NUM', 'PART', 'PDEMFP', 'PDEMFS', 'PDEMMP', 'PDEMMS', 'PINDFP', 'PINDFS', 'PINDMP', 'PINDMS', 'PINTFS', 'PPER1S', 'PPER2S', 'PPER3FP', 'PPER3FS', 'PPER3MP', 'PPER3MS', 'PPOBJFP', 'PPOBJFS', 'PPOBJMP', 'PPOBJMS', 'PREF', 'PREFP', 'PREFS', 'PREL', 'PRELFP', 'PRELFS', 'PRELMP', 'PRELMS', 'PREP', 'PRON', 'PROPN', 'PUNCT', 'SYM', 'VERB', 'VPPFP', 'VPPFS', 'VPPMP', 'VPPMS', 'VPPRE', 'X', 'XFAMIL', 'YPFOR', '_']\n"
          ]
        }
      ],
      "source": [
        "states = sorted(tag_counts.keys())\n",
        "print(f\"Number of POS tags (number of 'states'): {len(states)}\")\n",
        "print(\"View these POS tags (states)\")\n",
        "print(states)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s5rbeNRkv1TR",
        "outputId": "9f9c7c8c-ec75-44c1-f06d-986cf043b847"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "transition examples: \n",
            "(('--s--', 'DET'), 1)\n",
            "(('DET', 'NFP'), 4162)\n",
            "(('NFP', 'ADJFP'), 1452)\n",
            "\n",
            "emission examples: \n",
            "(('ADJMP', 'autres'), 135)\n",
            "(('ADJMP', 'petits'), 29)\n",
            "(('NMP', 'fruits'), 9)\n",
            "\n",
            "ambiguous word example: \n",
            "('ADJMS', 'comparable') 1\n",
            "('ADJFS', 'comparable') 4\n"
          ]
        }
      ],
      "source": [
        "print(\"transition examples: \")\n",
        "for ex in list(transition_counts.items())[:3]:\n",
        "    print(ex)\n",
        "print()\n",
        "\n",
        "print(\"emission examples: \")\n",
        "for ex in list(emission_counts.items())[200:203]:\n",
        "    print (ex)\n",
        "print()\n",
        "\n",
        "print(\"ambiguous word example: \")\n",
        "for tup,cnt in emission_counts.items():\n",
        "    if tup[1] == 'comparable':\n",
        "        print (tup, cnt) "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "fzPxi_puv1TR"
      },
      "source": [
        "**TESTING**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L225gYHMv1TS"
      },
      "outputs": [],
      "source": [
        "def predict_pos(prep, y, emission_counts, vocab, states):\n",
        "    '''\n",
        "    Input: \n",
        "        prep: a preprocessed version of 'y'. A list with the 'word' component of the tuples.\n",
        "        y: a corpus composed of a list of tuples where each tuple consists of (word, POS)\n",
        "        emission_counts: a dictionary where the keys are (tag,word) tuples and the value is the count\n",
        "        vocab: a dictionary where keys are words in vocabulary and value is an index\n",
        "        states: a sorted list of all possible tags for this assignment\n",
        "    Output: \n",
        "        accuracy: Number of times you classified a word correctly\n",
        "    '''\n",
        "    \n",
        "    # Initialize the number of correct predictions to zero\n",
        "    num_correct = 0\n",
        "    \n",
        "    # Get the (tag, word) tuples, stored as a set\n",
        "    all_words = set(emission_counts.keys())\n",
        "    \n",
        "    # Get the number of (word, POS) tuples in the corpus 'y'\n",
        "    total = len(y)\n",
        "    for word, y_tup in zip(prep, y): \n",
        "\n",
        "        # Split the (word, POS) string into a list of two items\n",
        "        y_tup_l = y_tup.split()\n",
        "        \n",
        "        # Verify that y_tup contain both word and POS\n",
        "        if len(y_tup_l) == 2:\n",
        "            \n",
        "            # Set the true POS label for this word\n",
        "            true_label = y_tup_l[1]\n",
        "\n",
        "        else:\n",
        "            # If the y_tup didn't contain word and POS, go to next word\n",
        "            continue\n",
        "    \n",
        "        count_final = 0\n",
        "        pos_final = ''\n",
        "        \n",
        "        # If the word is in the vocabulary...\n",
        "        if word in vocab:\n",
        "            for pos in states:\n",
        "                        \n",
        "                # define the key as the tuple containing the POS and word\n",
        "                key = (pos,word)\n",
        "\n",
        "                # check if the (pos, word) key exists in the emission_counts dictionary\n",
        "                if key in emission_counts: # complete this line\n",
        "\n",
        "                # get the emission count of the (pos,word) tuple \n",
        "                    count = emission_counts[key]\n",
        "\n",
        "                    # keep track of the POS with the largest count\n",
        "                    if count>count_final: # complete this line\n",
        "\n",
        "                        # update the final count (largest count)\n",
        "                        count_final = count\n",
        "\n",
        "                        # update the final POS\n",
        "                        pos_final = pos\n",
        "\n",
        "            # If the final POS (with the largest count) matches the true POS:\n",
        "            if pos_final == true_label: # complete this line\n",
        "                \n",
        "                # Update the number of correct predictions\n",
        "                num_correct += 1\n",
        "            \n",
        "    ### END CODE HERE ###\n",
        "    accuracy = num_correct / total\n",
        "    \n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vYuxW_7lv1TS",
        "outputId": "2672d727-1c4a-46e3-8e4b-f2d48d758913"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of prediction using predict_pos is 0.7606\n"
          ]
        }
      ],
      "source": [
        "accuracy_predict_pos = predict_pos(prep, y, emission_counts, vocab, states)\n",
        "print(f\"Accuracy of prediction using predict_pos is {accuracy_predict_pos:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
